# EcoSort
we use YOLO8n model for object detection for the task of garbage detection. 
doc: https://docs.ultralytics.com/tasks/detect/#models

#חלק 1: ייבוא ספריות והגדרת חומרה (שורות 1-48)

זהו שלב ה"הכנה". כאן אנחנו מביאים את הכלים הדרושים ובודקים על איזה מחשב אנחנו רצים.

import cv2, numpy, YOLO: טעינת כלי העבודה. cv2 לטיפול בתמונות , ו-YOLO הוא המוח שמבצע את הזיהוי.


הבלוק של try... except (שורות 22-48): זהו קטע קריטי לפיתוח.

try: הקוד מנסה לטעון את הספריות האמיתיות של הרובוט (ArmIK, HiwonderSDK). זה יעבוד רק אם הקוד רץ על ה-Raspberry Pi של הרובוט.

except ImportError: אם הקוד רץ על הלפטופ שלך (שאין לו זרוע רובוטית מחוברת), הוא ייכנס לפה.

MockBoard / MockArmIK: כאן יצרתי "רכיבים מזויפים". כשהקוד יגיד "תזיז מנוע", הרכיב המזויף פשוט ידפיס למסך "זזתי", במקום לקרוס עם שגיאה. זה מאפשר לך לבדוק את הלוגיקה בבית בלי הרובוט.

חלק 2: הגדרות וקבועים (שורות 50-81)

כאן אנחנו מגדירים את ה"חוקים" של המערכת.

MODEL_PATH = "yolov8n.pt": הנתיב לקובץ המוח של המודל. כרגע זה המודל הכללי.

CONF_THRESHOLD = 0.5: הרף התחתון. אם המודל בטוח פחות מ-50%, אנחנו מתעלמים.

WASTE_CLASSES (שורות 63-67): השינוי הגדול שעשיתי. במקום משתנה בודד, זהו מילון שממפה שמות (כמו 'plastic') למספרים (IDs).

כרגע שמתי שם IDs של COCO (בקבוק=39, ספר=73).

בעתיד, כשתאמני מודל על הדאטה-סטים שמצאת (Kaggle/Roboflow), את רק תשני את המספרים פה (למשל plastic: 0).

BIN_COORDS: איפה נמצאים הפחים? פח הפלסטיק במיקום X, ופח הנייר במיקום Y.

חלק 3: המחלקה WasteDetector - העיניים (שורות 85-143)

המחלקה הזו אחראית על הראייה.

__init__: הפונקציה שמופעלת כשהמחלקה נוצרת. היא טוענת את המודל לזיכרון.

detect_waste(self, image_path, target_type): הפונקציה הראשית. היא מקבלת תמונה וסוג פסולת לחיפוש (למשל 'plastic').

cv2.imread: קוראת את קובץ התמונה והופכת אותו למספרים. הוספתי בדיקה (if frame is None) כדי למנוע קריסה אם התמונה לא תקינה.

target_id = WASTE_CLASSES.get(...): שולף מהמילון את המספר שמתאים לסוג הפסולת (למשל, אם ביקשנו פלסטיק, הוא ישלוף 39).

self.model(frame): כאן קורה הקסם. המודל סורק את התמונה.

הלולאה for box in boxes: עוברים על כל מה שהמודל מצא.

התנאי if conf >= ... and cls_id == target_id: המסננת. שומרים את האובייקט רק אם הוא בטוח מספיק וגם הוא מהסוג שביקשנו.

detections.append(...): שומרים את המידע החשוב (מיקום וסוג) לרשימה שתוחזר בסוף.

חלק 4: המחלקה CoordinateMapper - המתורגמן (שורות 148-165)

המחלקה שממירה פיקסלים (תמונה דו-ממדית) לסנטימטרים (עולם תלת-ממדי).

pixel_to_world: הנוסחה פה מבצעת "נרמול" (Normalization).

היא בודקת איפה הפיקסל נמצא ביחס לרוחב התמונה (למשל, באמצע = 0.5).

ואז היא מכפילה את זה ברוחב השולחן בסנטימטרים (x_range_cm).

world_z = 2.0: אנחנו מניחים שהפסולת תמיד מונחת על השולחן, בגובה 2 ס"מ.

חלק 5: המחלקה RobotController - הידיים (שורות 170-217)

המחלקה הזו שולטת פיזית במנועים.

pickup_item: סדר הפעולות להרמה:

פתח צבת (GRIPPER_OPEN).

סע לנקודה שמעל החפץ (z + 6) – כדי לא להתנגש בו מהצד.

רד למטה (z) – אל החפץ עצמו.

סגור צבת (GRIPPER_CLOSE).

תרים את היד למעלה.

place_item: לוקח את החפץ למיקום של הפח (bin_coords), פותח את הצבת, וחוזר למצב התחלתי (reset_pose).

חלק 6: ה-Main - המנהל הראשי (שורות 222-269)

כאן הכל מתחבר.

אתחול: יוצר את שלושת האובייקטים (detector, mapper, robot).

לולאת הקבצים: עובר תמונה-תמונה בתיקייה.

detector.detect_waste(...): שולח את התמונה לזיהוי.

if detections:: אם נמצאה פסולת:

לוקח את הזיהוי הכי בטוח (ממיין לפי confidence).

mapper.pixel_to_world: ממיר את הפיקסל למיקום לרובוט.

robot.pickup_item: מפעיל את הזרוע.

robot.place_item: זורק לפח המתאים (לפי המיקום שהוגדר ב-BIN_COORDS).

זהו! הקוד הזה בנוי בצורה מודולרית, כך שאם מחר תחליפי מצלמה, תשני רק את CoordinateMapper, ואם תחליפי מודל זיהוי, תשני רק את WasteDetector או את המילון WASTE_CLASSES.
